{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c6a6cd6-9c2c-4dad-9e24-da8acf709d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7646357b-623f-4f95-8a26-d88bff81ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d1909757-6edc-4ae0-b220-8b9736aa1ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bb3649bb-66e1-4941-8708-cd3582e15e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 8]) torch.Size([182625])\n",
      "torch.Size([22655, 8]) torch.Size([22655])\n",
      "torch.Size([22866, 8]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "block_size = 8 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9d660149-1143-4ccc-91d8-b623ebf6aace",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5\n",
    "        self.bias = torch.zeros(fan_out) if bias else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        if self.bias != None:\n",
    "            return self.weight + self.bias\n",
    "        return self.weight\n",
    "\n",
    "class BatchNorm1D:\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "\n",
    "        if self.training:\n",
    "            xmean = x.mean(0, keepdim=True)\n",
    "            xvar = x.var(0, keepdim=True)\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "        \n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Embedding:\n",
    "    def __init__(self, num_embeddings, embeddings_dim):\n",
    "        self.weight = torch.randn((num_embeddings, embeddings_dim))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.out = self.weight[x]\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight]\n",
    "\n",
    "# Not equal to Flatten in pytorch\n",
    "class Flatten:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "\n",
    "    def __call__(self, x):\n",
    "        fD, sD, tD = x.shape\n",
    "        x = x.view(fD, sD//self.n, tD*self.n)\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.squeeze(1)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Sequential:\n",
    "  \n",
    "  def __init__(self, layers):\n",
    "    self.layers = layers\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [p for layer in self.layers for p in layer.parameters()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a9c3039b-8cb0-4a24-bbfd-20ed804706ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76552\n"
     ]
    }
   ],
   "source": [
    "n_embd = 24\n",
    "n_hidden_layers = 128\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, n_embd),\n",
    "    Flatten(2), Linear(n_embd * 2, n_hidden_layers, bias=False), BatchNorm1D(n_hidden_layers), Tanh(),\n",
    "    Flatten(2), Linear(n_hidden_layers * 2, n_hidden_layers, bias=False), BatchNorm1D(n_hidden_layers), Tanh(),\n",
    "    Flatten(2), Linear(n_hidden_layers * 2, n_hidden_layers, bias=False), BatchNorm1D(n_hidden_layers), Tanh(),\n",
    "    Linear(n_hidden_layers, vocab_size),\n",
    "])\n",
    "\n",
    "with torch.no_grad():\n",
    "  model.layers[-1].weight *= 0.1 # last layer make less confident\n",
    "\n",
    "parameters = model.parameters()\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd72338a-6273-46e4-8570-c0d4e0cb0593",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m100000\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.01\u001b[39m \u001b[38;5;66;03m# step learning rate decay\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters:\n\u001b[0;32m---> 21\u001b[0m     p\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m-\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# track stats\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m# print every once in a while\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size, ))\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "    \n",
    "\n",
    "    logits = model(Xb)        \n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "  \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c31a9e0-0e7f-45ec-932b-9e566bb58237",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "695aa486-d754-4b12-8326-06c801a451d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 3.103217363357544\n",
      "val 3.1043572425842285\n"
     ]
    }
   ],
   "source": [
    "# evaluate the loss\n",
    "@torch.no_grad() # this decorator disables gradient tracking inside pytorch\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  logits = model(x)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "337ef757-47cc-4277-abc9-a51ad0e29eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.\n",
      "y.\n",
      "igkeborym.\n",
      "xu.\n",
      "fesijyvbuo.\n",
      "tgauykxtrn.\n",
      ".\n",
      "cprjgtodclqsdwrnsz.\n",
      "blyh.\n",
      "dvzhoefvdwyixai.\n",
      "maolreshxf.\n",
      "eyjdqjjeapyqda.\n",
      "de.\n",
      "khxtlbhushihfgeqldgzvnyd.\n",
      ".\n",
      "uxxdnyenyey.\n",
      "awdnlxrwx.\n",
      "dfvhvpdyxewu.\n",
      "julqwfujiyjbc.\n",
      "bzolrsrgkr.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        # forward pass the neural net\n",
    "        logits = model(torch.tensor([context]))\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        # sample from the distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1).item()\n",
    "        # shift the context window and track the samples\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        # if we sample the special '.' token, break\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(\"\".join(itos[i] for i in out))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "460fe689-1663-4894-a904-2299de91b370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.2512e-01, -2.2393e-01, -1.4633e+00, -2.3942e+00,  1.7683e-01,\n",
       "          -1.4251e-01, -2.3589e-01,  4.3456e-01, -4.7320e-01,  2.1212e-01,\n",
       "           2.1429e-01,  2.5476e-01,  7.3470e-01,  6.2176e-01, -3.5796e-01,\n",
       "          -2.3217e+00,  1.6595e-01, -2.7017e-01, -9.2860e-01, -3.3029e-02],\n",
       "         [-6.5910e-01,  6.1995e-01, -7.2287e-01, -2.0508e+00,  1.7791e+00,\n",
       "           7.9939e-01, -2.4499e-01,  1.2096e-01, -8.5495e-01, -6.1114e-01,\n",
       "          -2.1237e-01, -3.9972e-01,  6.8885e-04,  1.5386e+00, -1.0314e+00,\n",
       "          -1.1214e-01,  8.1772e-01,  4.3274e-01,  3.8807e-01,  7.6808e-01],\n",
       "         [ 6.5319e-01,  6.6067e-01,  1.2611e+00, -1.4447e-01,  1.5423e+00,\n",
       "          -5.1042e-01, -3.7211e-01,  5.9883e-01,  1.6525e+00, -1.0708e+00,\n",
       "          -3.4717e-01, -1.7310e-01,  3.7033e-01, -2.1152e+00, -4.1501e-01,\n",
       "          -4.3801e-02, -6.7324e-01, -1.0372e-01, -7.4317e-01,  7.7065e-01],\n",
       "         [ 9.4933e-01,  8.3428e-01,  9.5943e-02,  1.0666e-01, -3.4945e-01,\n",
       "          -1.0201e+00,  5.4044e-01,  1.2080e+00,  4.1629e-01, -2.8733e-01,\n",
       "           7.3594e-02,  2.7981e-01,  7.5711e-01, -1.3864e+00, -3.0150e-01,\n",
       "          -5.5419e-03, -8.2058e-02, -3.6119e-01, -2.1969e-01, -6.1715e-01]],\n",
       "\n",
       "        [[ 1.0594e+00,  4.6712e-01, -1.6471e-01,  1.6356e+00, -1.1076e+00,\n",
       "           6.3350e-01,  6.7185e-01,  2.0265e+00, -3.8004e-01,  1.0523e+00,\n",
       "          -2.8248e-01,  5.1318e-02,  1.6235e-02,  1.8265e-01, -6.5642e-01,\n",
       "           6.9966e-02, -3.2510e-01,  1.9201e+00,  3.2181e-01,  2.4780e+00],\n",
       "         [-3.5259e-01, -1.1596e+00, -1.7459e+00,  1.1663e+00,  4.6609e-01,\n",
       "           7.3451e-01, -1.0615e+00, -9.9821e-02,  7.9532e-01, -9.4672e-02,\n",
       "           5.8641e-01, -1.0465e+00,  1.6592e-01, -2.0852e-01,  1.2525e+00,\n",
       "          -8.3820e-01, -1.2783e+00,  7.0895e-01, -2.3290e-01,  8.1089e-01],\n",
       "         [ 1.8676e+00, -1.4140e+00, -9.5902e-01, -5.6171e-01, -8.9182e-01,\n",
       "          -5.3739e-01,  8.0628e-01,  1.0356e+00,  3.7447e+00, -8.5215e-01,\n",
       "          -1.2945e+00, -1.2013e+00,  9.4405e-02, -1.0455e+00, -1.4972e+00,\n",
       "           9.2431e-01,  6.9953e-01,  4.3220e-01, -5.9732e-02, -1.4291e-01],\n",
       "         [ 9.3790e-01, -3.3869e-01, -9.1107e-01, -6.1123e-01, -1.5347e+00,\n",
       "          -6.7873e-01,  1.6268e+00, -6.2707e-02, -6.2399e-01, -1.9485e+00,\n",
       "           1.5288e+00,  8.8012e-02, -1.3070e+00,  1.9863e-01,  5.5838e-02,\n",
       "           3.1665e+00,  1.3396e-01,  2.4693e+00, -6.0929e-01, -1.5044e-01]],\n",
       "\n",
       "        [[-1.3548e+00, -3.7327e-01,  1.1519e+00, -1.3097e-01, -1.0971e+00,\n",
       "           6.4323e-01,  1.7266e+00, -8.8074e-01,  9.7809e-01,  5.1883e-01,\n",
       "          -3.6341e-01, -3.7822e-01, -3.8384e-01, -1.3399e+00,  1.0847e+00,\n",
       "           3.0492e-01,  1.8371e-02, -4.9915e-01, -5.9428e-01, -1.5484e+00],\n",
       "         [ 1.2273e-02,  5.4627e-02, -3.5345e-01,  7.7446e-01,  3.1199e-01,\n",
       "          -1.0484e+00, -1.3337e+00,  2.5860e-01,  8.3231e-01,  1.8562e+00,\n",
       "           1.1391e+00, -6.5986e-01,  4.7059e-01, -1.2617e+00,  1.7517e+00,\n",
       "          -8.4358e-01, -1.3268e+00, -6.8912e-01, -4.5976e-01, -3.0875e-01],\n",
       "         [ 5.5264e-01, -1.0561e+00,  5.6548e-01, -1.0968e+00,  2.7210e+00,\n",
       "           1.3903e+00,  6.5823e-01, -2.9315e-01,  2.4846e-02, -1.2068e+00,\n",
       "           6.4916e-01,  1.1267e+00, -1.1315e+00, -5.7286e-01, -9.0267e-01,\n",
       "           1.8628e+00, -1.4996e+00,  7.3112e-02,  6.0847e-01, -1.0897e+00],\n",
       "         [ 1.0548e+00, -1.2734e+00,  1.2492e+00, -1.0500e+00,  2.0062e+00,\n",
       "          -5.8698e-01, -7.6802e-01,  2.8554e+00, -1.0936e+00, -1.4136e+00,\n",
       "           1.6125e-01, -7.0835e-01, -5.3359e-01, -5.8572e-01,  6.2117e-01,\n",
       "          -8.7312e-01,  3.5916e-02, -6.5061e-01, -4.7632e-01, -2.6329e+00]],\n",
       "\n",
       "        [[-7.0469e-01,  7.0873e-02,  6.7542e-02, -7.1831e-01,  1.1687e-01,\n",
       "           4.5261e-01,  3.6280e-01,  1.2031e+00, -2.3265e+00,  2.1391e+00,\n",
       "           6.1391e-01, -1.9889e-01,  4.4181e-01,  6.3691e-01,  1.8537e-01,\n",
       "           5.7909e-01,  2.4807e+00,  8.9050e-01,  2.4451e-01,  1.1362e+00],\n",
       "         [-1.5554e+00, -6.4908e-01, -3.1888e+00, -4.7041e-01,  8.6423e-01,\n",
       "           1.1296e+00, -1.2058e+00, -1.3345e+00, -6.9296e-01,  6.9698e-01,\n",
       "           1.1950e+00, -1.4712e+00,  3.2956e-01,  1.2330e+00,  2.0809e+00,\n",
       "           1.3624e-01,  9.5912e-01, -4.6407e-03, -1.4929e+00,  9.8077e-01],\n",
       "         [-7.9671e-01,  2.8966e-01, -1.3270e+00, -5.8707e-01, -2.0253e+00,\n",
       "           7.7513e-01,  5.5457e-01,  3.1952e-01, -1.4128e+00,  5.3568e-01,\n",
       "          -1.0447e+00,  1.0701e+00,  1.2644e+00,  9.3754e-01,  1.4487e+00,\n",
       "          -1.2759e-01,  8.2870e-01,  1.5552e+00,  5.1939e-01,  1.9744e+00],\n",
       "         [ 1.0152e+00, -2.5820e-01,  1.5595e+00,  8.3497e-01, -1.9447e+00,\n",
       "           1.5550e+00,  6.2716e-01,  7.2949e-01,  8.5292e-01, -1.1492e+00,\n",
       "           3.5844e-01, -1.1888e-01,  9.9280e-02, -1.7273e+00,  6.4605e-01,\n",
       "           5.1695e-02,  5.9752e-01, -9.9346e-02,  1.2805e-01,  1.1285e+00]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = torch.randn(4, 8, 10)\n",
    "e.view(4, 4, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89602e81-ceb0-436e-91ec-5553c0ee3c62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
